{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 机器学习纳米学位\n",
    "## 监督学习\n",
    "## 项目2: 为*CharityML*寻找捐献者"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欢迎来到机器学习工程师纳米学位的第二个项目！在此文件中，有些示例代码已经提供给你，但你还需要实现更多的功能让项目成功运行。除非有明确要求，你无须修改任何已给出的代码。以**'练习'**开始的标题表示接下来的代码部分中有你必须要实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以'TODO'标出。请仔细阅读所有的提示！\n",
    "\n",
    "除了实现代码外，你还必须回答一些与项目和你的实现有关的问题。每一个需要你回答的问题都会以**'问题 X'**为标题。请仔细阅读每个问题，并且在问题后的**'回答'**文字框中写出完整的答案。我们将根据你对问题的回答和撰写代码所实现的功能来对你提交的项目进行评分。\n",
    ">**提示：**Code 和 Markdown 区域可通过**Shift + Enter**快捷键运行。此外，Markdown可以通过双击进入编辑模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始\n",
    "\n",
    "在这个项目中，你将使用1994年美国人口普查收集的数据，选用几个监督学习算法以准确地建模被调查者的收入。然后，你将根据初步结果从中选择出最佳的候选算法，并进一步优化该算法以最好地建模这些数据。你的目标是建立一个能够准确地预测被调查者年收入是否超过50000美元的模型。这种类型的任务会出现在那些依赖于捐款而存在的非营利性组织。了解人群的收入情况可以帮助一个非营利性的机构更好地了解他们要多大的捐赠，或是否他们应该接触这些人。虽然我们很难直接从公开的资源中推断出一个人的一般收入阶层，但是我们可以（也正是我们将要做的）从其他的一些公开的可获得的资源中获得一些特征从而推断出该值。\n",
    "\n",
    "这个项目的数据集来自[UCI机器学习知识库](https://archive.ics.uci.edu/ml/datasets/Census+Income)。这个数据集是由Ron Kohavi和Barry Becker在发表文章_\"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid\"_之后捐赠的，你可以在Ron Kohavi提供的[在线版本](https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf)中找到这个文章。我们在这里探索的数据集相比于原有的数据集有一些小小的改变，比如说移除了特征`'fnlwgt'` 以及一些遗失的或者是格式不正确的记录。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 探索数据\n",
    "运行下面的代码单元以载入需要的Python库并导入人口普查数据。注意数据集的最后一列`'income'`将是我们需要预测的列（表示被调查者的年收入会大于或者是最多50,000美元），人口普查数据中的每一列都将是关于被调查者的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为这个项目导入需要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display # 允许为DataFrame使用display()\n",
    "\n",
    "# 导入附加的可视化代码visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "# 为notebook提供更加漂亮的可视化\n",
    "%matplotlib inline\n",
    "\n",
    "# 导入人口普查数据\n",
    "data = pd.read_csv(\"census.csv\")\n",
    "#print data\n",
    "# 成功 - 显示第一条记录\n",
    "display(data.head(n=1))\n",
    "#data.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：数据探索\n",
    "首先我们对数据集进行一个粗略的探索，我们将看看每一个类别里会有多少被调查者？并且告诉我们这些里面多大比例是年收入大于50,000美元的。在下面的代码单元中，你将需要计算以下量：\n",
    "\n",
    "- 总的记录数量，`'n_records'`\n",
    "- 年收入大于50,000美元的人数，`'n_greater_50k'`.\n",
    "- 年收入最多为50,000美元的人数 `'n_at_most_50k'`.\n",
    "- 年收入大于50,000美元的人所占的比例， `'greater_percent'`.\n",
    "\n",
    "**提示：** 您可能需要查看上面的生成的表，以了解`'income'`条目的格式是什么样的。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO：总的记录数\n",
    "n_records = len(data)\n",
    "\n",
    "# TODO：被调查者的收入大于$50,000的人数\n",
    "n_greater_50k = len(data[data['income']=='>50K'])\n",
    "#n_greater_50k = data[data['income']=='>50K'].shape[0]  #索引这部分相同元素的数据df，然后查看该第一维的长度\n",
    "\n",
    "# TODO：被调查者的收入最多为$50,000的人数\n",
    "n_at_most_50k = len(data[data['income']=='<=50K'])\n",
    "\n",
    "# TODO：被调查者收入大于$50,000所占的比例\n",
    "greater_percent = 100.0*n_greater_50k/n_records\n",
    "greater_percent = float(n_greater_50k)/n_records*100\n",
    "\n",
    "# 打印结果\n",
    "print \"Total number of records: {}\".format(n_records)\n",
    "print \"Individuals making more than $50,000: {}\".format(n_greater_50k)\n",
    "print \"Individuals making at most $50,000: {}\".format(n_at_most_50k)\n",
    "print \"Percentage of individuals making more than $50,000: {:.2f}%\".format(greater_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 准备数据\n",
    "在数据能够被作为输入提供给机器学习算法之前，它经常需要被清洗，格式化，和重新组织 - 这通常被叫做**预处理**。幸运的是，对于这个数据集，没有我们必须处理的无效或丢失的条目，然而，由于某一些特征存在的特性我们必须进行一定的调整。这个预处理都可以极大地帮助我们提升几乎所有的学习算法的结果和预测能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换倾斜的连续特征\n",
    "\n",
    "一个数据集有时可能包含至少一个靠近某个数字的特征，但有时也会有一些相对来说存在极大值或者极小值的不平凡分布的的特征。算法对这种分布的数据会十分敏感，并且如果这种数据没有能够很好地规一化处理会使得算法表现不佳。在人口普查数据集的两个特征符合这个描述：'`capital-gain'`和`'capital-loss'`。\n",
    "\n",
    "运行下面的代码单元以创建一个关于这两个特征的条形图。请注意当前的值的范围和它们是如何分布的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将数据切分成特征和对应的标签\n",
    "income_raw = data['income']\n",
    "print income_raw.shape\n",
    "features_raw = data.drop('income', axis = 1)# axis=0表示列(式），axis=1表示行（式），此处代表从左到右排列的这些series找到'income'series\n",
    "# 可视化原来数据的倾斜的连续特征\n",
    "vs.distribution(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于高度倾斜分布的特征如`'capital-gain'`和`'capital-loss'`，常见的做法是对数据施加一个<a href=\"https://en.wikipedia.org/wiki/Data_transformation_(statistics)\">对数转换</a>，将数据转换成对数，这样非常大和非常小的值不会对学习算法产生负面的影响。并且使用对数变换显著降低了由于异常值所造成的数据范围异常。但是在应用这个变换时必须小心：因为0的对数是没有定义的，所以我们必须先将数据处理成一个比0稍微大一点的数以成功完成对数转换。\n",
    "\n",
    "运行下面的代码单元来执行数据的转换和可视化结果。再次，注意值的范围和它们是如何分布的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于倾斜的数据使用Log转换\n",
    "skewed = ['capital-gain', 'capital-loss']\n",
    "features_raw[skewed] = data[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "# 可视化经过log之后的数据分布\n",
    "vs.distribution(features_raw, transformed = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 规一化数字特征\n",
    "除了对于高度倾斜的特征施加转换，对数值特征施加一些形式的缩放通常会是一个好的习惯。在数据上面施加一个缩放并不会改变数据分布的形式（比如上面说的'capital-gain' or 'capital-loss'）；但是，规一化保证了每一个特征在使用监督学习器的时候能够被平等的对待。注意一旦使用了缩放，观察数据的原始形式不再具有它本来的意义了，就像下面的例子展示的。\n",
    "\n",
    "运行下面的代码单元来规一化每一个数字特征。我们将使用[`sklearn.preprocessing.MinMaxScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)来完成这个任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入sklearn.preprocessing.StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 初始化一个 scaler，并将它施加到特征上\n",
    "scaler = MinMaxScaler()\n",
    "numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "features_raw[numerical] = scaler.fit_transform(data[numerical])\n",
    "# 显示一个经过缩放的样例记录\n",
    "display(features_raw.head(n = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：数据预处理\n",
    "\n",
    "从上面的**数据探索**中的表中，我们可以看到有几个属性的每一条记录都是非数字的。通常情况下，学习算法期望输入是数字的，这要求非数字的特征（称为类别变量）被转换。转换类别变量的一种流行的方法是使用**独热编码**方案。独热编码为每一个非数字特征的每一个可能的类别创建一个_“虚拟”_变量。例如，假设`someFeature`有三个可能的取值`A`，`B`或者`C`，。我们将把这个特征编码成`someFeature_A`, `someFeature_B`和`someFeature_C`.\n",
    "\n",
    "|   | 一些特征 |                    | 特征_A | 特征_B | 特征_C |\n",
    "| :-: | :-: |                            | :-: | :-: | :-: |\n",
    "| 0 |  B  |  | 0 | 1 | 0 |\n",
    "| 1 |  C  | ----> 独热编码 ----> | 0 | 0 | 1 |\n",
    "| 2 |  A  |  | 1 | 0 | 0 |\n",
    "\n",
    "此外，对于非数字的特征，我们需要将非数字的标签`'income'`转换成数值以保证学习算法能够正常工作。因为这个标签只有两种可能的类别（\"<=50K\"和\">50K\"），我们不必要使用独热编码，可以直接将他们编码分别成两个类`0`和`1`，在下面的代码单元中你将实现以下功能：\n",
    " - 使用[`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies)对`'features_raw'`数据来施加一个独热编码。\n",
    " - 将目标标签`'income_raw'`转换成数字项。\n",
    "   - 将\"<=50K\"转换成`0`；将\">50K\"转换成`1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO：使用pandas.get_dummies()对'features_raw'数据进行独热编码\n",
    "features = pd.get_dummies(features_raw)#.astype(int)\n",
    "#print len(features.columns)\n",
    "# TODO：将'income_raw'编码成数字值\n",
    "\n",
    "#income = income_raw.copy()  #apply已经默认开副本了。\n",
    "income = income_raw.apply(lambda x : 1 if x == '>50K' else 0)\n",
    "\n",
    "#income[income == '<=50K'] = 0\n",
    "#income[income == '>50K'] = 1\n",
    "#income = income.astype(int)\n",
    "\n",
    "# 打印经过独热编码之后的特征数量\n",
    "\n",
    "encoded = list(features.columns)\n",
    "print \"{} total features after one-hot encoding.\".format(len(encoded))\n",
    "\n",
    "# 移除下面一行的注释以观察编码的特征名字\n",
    "print encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混洗和切分数据\n",
    "现在所有的 _类别变量_ 已被转换成数值特征，而且所有的数值特征已被规一化。和我们一般情况下做的一样，我们现在将数据（包括特征和它们的标签）切分成训练和测试集。其中80%的数据将用于训练和20%的数据用于测试。\n",
    "\n",
    "运行下面的代码单元来完成切分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 将'features'和'income'数据切分成训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, income, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# 显示切分的结果\n",
    "print \"Training set has {} samples.\".format(X_train.shape[0])\n",
    "print \"Testing set has {} samples.\".format(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 评价模型性能\n",
    "在这一部分中，我们将尝试四种不同的算法，并确定哪一个能够最好地建模数据。这里面的三个将是你选择的监督学习器，而第四种算法被称为一个*朴素的预测器*。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评价方法和朴素的预测器\n",
    "*CharityML*通过他们的研究人员知道被调查者的年收入大于\\$50,000最有可能向他们捐款。因为这个原因*CharityML*对于准确预测谁能够获得\\$50,000以上收入尤其有兴趣。这样看起来使用**准确率**作为评价模型的标准是合适的。另外，把*没有*收入大于\\$50,000的人识别成年收入大于\\$50,000对于*CharityML*来说是有害的，因为他想要找到的是有意愿捐款的用户。这样，我们期望的模型具有准确预测那些能够年收入大于\\$50,000的能力比模型去**查全**这些被调查者*更重要*。我们能够使用**F-beta score**作为评价指标，这样能够同时考虑查准率和查全率：\n",
    "\n",
    "$$ F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall} $$\n",
    "\n",
    "\n",
    "尤其是，当$\\beta = 0.5$的时候更多的强调查准率，这叫做**F$_{0.5}$ score** （或者为了简单叫做F-score）。\n",
    "\n",
    "通过查看不同类别的数据分布（那些最多赚\\$50,000和那些能够赚更多的），我们能发现：很明显的是很多的被调查者年收入没有超过\\$50,000。这点会显著地影响**准确率**，因为我们可以简单地预测说*“这个人的收入没有超过\\$50,000”*，这样我们甚至不用看数据就能做到我们的预测在一般情况下是正确的！做这样一个预测被称作是**朴素的**，因为我们没有任何信息去证实这种说法。通常考虑对你的数据使用一个*朴素的预测器*是十分重要的，这样能够帮助我们建立一个模型的表现是否好的基准。那有人说，使用这样一个预测是没有意义的：如果我们预测所有人的收入都低于\\$50,000，那么*CharityML*就不会有人捐款了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 1 - 朴素预测器的性能\n",
    "*如果我们选择一个无论什么情况都预测被调查者年收入大于\\$50,000的模型，那么这个模型在这个数据集上的准确率和F-score是多少？*  \n",
    "**注意：** 你必须使用下面的代码单元将你的计算结果赋值给`'accuracy'` 和 `'fscore'`，这些值会在后面被使用，请注意这里不能使用scikit-learn，你需要根据公式自己实现相关计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO： 计算准确率\n",
    "accuracy = float(len(income[income == 1]))/(len(income[income == 1])+len(income[income == 0]))\n",
    "recall = 1.0*len(income[income == 1])/(len(income[income == 1])+0)\n",
    "beta = 0.5\n",
    "# TODO： 使用上面的公式，并设置beta=0.5计算F-score\n",
    "fscore = (1+beta**2) * accuracy * recall / (beta**2*accuracy+1)\n",
    "# 打印结果\n",
    "print \"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 监督学习模型\n",
    "**下面的监督学习模型是现在在** [`scikit-learn`](http://scikit-learn.org/stable/supervised_learning.html) **中你能够选择的模型**\n",
    "- 高斯朴素贝叶斯 (GaussianNB)\n",
    "- 决策树\n",
    "- 集成方法 (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n",
    "- K近邻 (KNeighbors)\n",
    "- 随机梯度下降分类器 (SGDC)\n",
    "- 支撑向量机 (SVM)\n",
    "- Logistic回归\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 2 - 模型应用\n",
    "\n",
    "列出从上面的监督学习模型中选择的三个适合我们这个问题的模型，你将在人口普查数据上测试这每个算法。对于你选择的每一个算法：\n",
    "\n",
    "- *描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）*\n",
    "- *这个模型的优势是什么？他什么情况下表现最好？*\n",
    "- *这个模型的缺点是什么？什么条件下它表现很差？*\n",
    "- *根据我们当前数据集的特点，为什么这个模型适合这个问题。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：\n",
    "\n",
    "决策树ID3算法：\n",
    "应用场景：因为它能够生成清晰的基于特征(feature)选择不同预测结果的树状结构，可以有效帮助理解手上的数据。但该算法最终在底层判断是基于单个条件的，容易被以很低成本恶意逃过算法监测。受限于它的简单性，决策树更大的用处是作为一些更有用的算法的基石。\n",
    "优点：\n",
    "（1）易于理解和解释；（2）对数据预处理依赖性低；（3）能够同时处理多种数据类型；\n",
    "该算法在处理离散数据集时表现最好\n",
    "缺点：\n",
    "（1）容易出现过拟合现象，尤其是特征量较多的时候，需要调整参数剪枝；（2）容易产生局部最优解；（3）只有一个深度参数，严格意义上不能发挥网格搜索方法的优势。\n",
    "该算法在处理连续性大数据集（如连续性字段）表现很差（因为叠加的特征多，深度计算的耗时长）\n",
    "\n",
    "选择这个模型是因为当前数据集特征量较少，该模型可以很好的发挥优势。\n",
    "---\n",
    "KNN：\n",
    "应用场景：需要一个特别容易解释的模型的时候。（比如需要向用户解释原因的推荐算法。）\n",
    "优点：\n",
    "（1）简单、有效；（2）重新训练的代价较低；（3）计算时间和空间线性于训练集的规模。（4）对数据没有假设，对异常值不敏感（5）可用于非线性分类\n",
    "该算法比较适用于数据集样本容量比较大的分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。\n",
    "缺点：\n",
    "（1）输出的可解释性不强；（2）较主动学习的算法而言，速度较慢；（3）样本不平衡对预测结论影响大；（4）需要大量的内存\n",
    "\n",
    "选择这个模型是因为本数据集样本不至于太少，且提供了几个特征，仅仅要求分2个类别，可能较适合。\n",
    "---\n",
    "SVM-SVC：\n",
    "应用场景：SVM在很多数据集上都有优秀的表现。相对来说，SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。和随机森林一样，这是一个拿到数据就可以先尝试一下的算法。\n",
    "优点：\n",
    "（1）可以解决小样本情况下的机器学习问题；（2）可提高泛化性能；（3）可解决高维问题；（4）可解决非线性问题；（5）无需依赖全部数据，计算较快。\n",
    "在解决非线性、高维问题时表现最好。\n",
    "缺点：\n",
    "（1）内存消耗大；（2）需要对数据提前归一化；（3）难以理解与解释；（4）当观测样本很多时，效率并不是很高；（5）对非线性问题没有通用解决方案，运行和调参较困难；（6）对缺失数据敏感，噪音过多的时候容易过拟合。\n",
    "海量数据集中，若类之间严重重叠，要考虑独立证据，该模型不太适合，因为耗时长，且表现不好。\n",
    "\n",
    "选择这个模型是因为本数据集样本并非海量，且不需要考虑独立证据。\n",
    "---\n",
    "Random Forest（后备）：\n",
    "应用场景：\n",
    "优点：\n",
    "（1）能够处理很高维度（feature很多）的数据，并且不用做特征选择；（2）训练完后，能够检测到feature间的互相影响，能给出哪些feature较重要；（3）在创建随机森林的时候，对generlization error使用的是无偏估计；（4）训练速度快；（5）容易做成并行化方法\n",
    "缺点：\n",
    "（1）在某些噪音较大的分类或回归问题上会过拟合；（2）取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值不可信\n",
    "选择这个模型是因为该模型对数据集没什么要求，面对同样的数据集，或许测试性能比决策树还要更佳。\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 - 创建一个训练和预测的流水线\n",
    "为了正确评估你选择的每一个模型的性能，创建一个能够帮助你快速有效地使用不同大小的训练集并在测试集上做预测的训练和测试的流水线是十分重要的。\n",
    "你在这里实现的功能将会在接下来的部分中被用到。在下面的代码单元中，你将实现以下功能：\n",
    "\n",
    " - 从[`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)中导入`fbeta_score`和`accuracy_score`。\n",
    " - 用样例训练集拟合学习器，并记录训练时间。\n",
    " - 用学习器来对训练集进行预测并记录预测时间。\n",
    " - 在最前面的300个*训练数据*上做预测。\n",
    " - 计算训练数据和测试数据的准确率。\n",
    " - 计算训练数据和测试数据的F-score。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO：从sklearn中导入两个评价指标 - fbeta_score和accuracy_score\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    results = {}\n",
    "\n",
    "    # TODO：使用sample_size大小的训练数据来拟合学习器\n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size'\n",
    "\n",
    "    start_train = time() # 获得程序开始时间\n",
    "    learner = learner.fit(X_train[:sample_size],y_train[:sample_size])\n",
    "    end_train = time() # 获得程序结束时间\n",
    "\n",
    "    # TODO：计算训练时间\n",
    "    results['train_time'] = end_train-start_train\n",
    "\n",
    "    # TODO: 得到在测试集上的预测值\n",
    "    #       然后得到对前300个训练数据的预测结果\n",
    "    start_test = time() # 获得程序开始时间\n",
    "    y_pred_test = learner.predict(X_test)\n",
    "    y_pred_train = learner.predict(X_train[:300])\n",
    "    end_test = time() # 获得程序结束时间\n",
    "\n",
    "    # TODO：计算预测用时\n",
    "    results['pred_time'] = end_test-start_test\n",
    "\n",
    "    # TODO：计算在最前面的300个训练数据的准确率\n",
    "    y_true_train = y_train[:300]\n",
    "    results['acc_train'] = accuracy_score(y_true_train, y_pred_train) \n",
    "\n",
    "    # TODO：计算在测试集上的准确率\n",
    "    results['acc_test'] = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    # TODO：计算在最前面300个训练数据上的F-score\n",
    "    results['f_train'] = fbeta_score(y_true_train, y_pred_train, beta=0.5)#不需要设置average参数，它是在多分类任务中使用的，二分类不需要。\n",
    "\n",
    "    # TODO：计算测试集上的F-score\n",
    "    results['f_test'] = fbeta_score(y_test, y_pred_test, beta=0.5)\n",
    "\n",
    "    # 成功\n",
    "    print \"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size)\n",
    "        \n",
    "    # 返回结果\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：初始模型的评估\n",
    "在下面的代码单元中，您将需要实现以下功能：             \n",
    "- 导入你在前面讨论的三个监督学习模型。             \n",
    "- 初始化三个模型并存储在`'clf_A'`，`'clf_B'`和`'clf_C'`中。         \n",
    "  - 如果可能对每一个模型都设置一个`random_state`。       \n",
    "  - **注意：**这里先使用每一个模型的默认参数，在接下来的部分中你将需要对某一个模型的参数进行调整。             \n",
    "- 计算记录的数目等于1%，10%，和100%的训练数据，并将这些值存储在`'samples'`中             \n",
    "\n",
    "**注意：**取决于你选择的算法，下面实现的代码可能需要一些时间来运行！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO：从sklearn中导入三个监督学习模型\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# TODO：初始化三个模型\n",
    "clf_A = DecisionTreeClassifier(random_state = 0)\n",
    "clf_B = KNeighborsClassifier()\n",
    "clf_C = SVC(random_state = 0)\n",
    "\n",
    "# TODO：计算1%， 10%， 100%的训练数据分别对应多少点\n",
    "samples_1 = int(0.01*len(y_train))\n",
    "samples_10 = int(0.1*len(y_train))\n",
    "samples_100 = int(len(y_train))\n",
    "\n",
    "# 收集学习器的结果\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[clf_name][i] = \\\n",
    "        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 对选择的三个模型得到的评价结果进行可视化\n",
    "vs.evaluate(results, accuracy, fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 提高效果\n",
    "\n",
    "在这最后一节中，您将从三个有监督的学习模型中选择*最好的*模型来使用学生数据。你将在整个训练集（`X_train`和`y_train`）上通过使用网格搜索优化至少调节一个参数以获得一个比没有调节之前更好的F-score。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 3 - 选择最佳的模型\n",
    "\n",
    "*基于你前面做的评价，用一到两段向*CharityML*解释这三个模型中哪一个对于判断被调查者的年收入大于\\$50,000是最合适的。*             \n",
    "**提示：**你的答案应该包括关于评价指标，预测/训练时间，以及该算法是否适合这里的数据的讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：DecisionTreeClassifier对于判断被调查者的年收入大于￥50,000是最合适的。\n",
    "-在模型表现上：从accuracy score和F-score来看，三种算法在训练集、测试集上的表现中,DecisionTreeClassifier的表现较好一些；\n",
    "-在计算成本上：DTClassifier的训练时间最少，SVC的训练时间最多；DTClassifier的测试时间最少，KNN的测试时间最多。综合考虑，DTClassifier的计算成本最低。\n",
    "-在算法是否适合数据集上：三种算法在训练集和测试集上的表现相近，证明数据中的噪音在这三个算法的性能影响较小。\n",
    "其中，KNN和SVC比DTClassifier的泛化能力好，DTClassifier或许存在过拟合，需要进一步地调整参数。**\n",
    "总的来说，SVC和KNN无论如何调整参数，都不能明显地把计算成本降低下来，且模型表现的上升空间较低，DTClassifier调整参数后却能提高模型的泛化能力，比KNN和SVC适合这里的数据讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 4 - 用通俗的话解释模型\n",
    "\n",
    "*用一到两段话，向*CharityML*用外行也听得懂的话来解释最终模型是如何工作的。你需要解释所选模型的主要特点。例如，这个模型是怎样被训练的，它又是如何做出预测的。避免使用高级的数学或技术术语，不要使用公式或特定的算法名词。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：\n",
    "这个模型主要是用来分类的。\n",
    "-类别少一点的就像区分苹果和雪梨，它事先观察了大量的苹果和雪梨，然后就记住了譬如出现黄色的很可能是雪梨，有雀斑的很可能是雪梨这样的观点。当我们再一次给他看苹果或雪梨的时候，他就会根据事先学习提取的特征标签，判断这究竟是苹果还是雪梨。\n",
    "-类别多一点的情况就像考试，不同的知识点、不同的应用方法配合起来就会出现很多很多的题型。这个模型呢，它自己事先看了很多很多套试卷，把不同题型考察什么知识点该什么方法答题全都给记住了。在真正到了考试的时候，它就看试卷都有哪些题型哪些考点，逐一地在回忆里匹对，把不相关的题型知识点一大块一小刀的逐渐排除，最后得到的就是它所认为最接近正确答案的预测。** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：模型调优\n",
    "调节选择的模型的参数。使用网格搜索（GridSearchCV）来至少调整模型的重要参数（至少调整一个），这个参数至少需给出并尝试3个不同的值。你要使用整个训练集来完成这个过程。在接下来的代码单元中，你需要实现以下功能：\n",
    "\n",
    "- 导入[`sklearn.model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)和[`sklearn.metrics.make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).\n",
    "- 初始化你选择的分类器，并将其存储在`clf`中。\n",
    " - 如果能够设置的话，设置`random_state`。\n",
    "- 创建一个对于这个模型你希望调整参数的字典。\n",
    " - 例如: parameters = {'parameter' : [list of values]}。\n",
    " - **注意：** 如果你的学习器（learner）有 `max_features` 参数，请不要调节它！\n",
    "- 使用`make_scorer`来创建一个`fbeta_score`评分对象（设置$\\beta = 0.5$）。\n",
    "- 在分类器clf上用'scorer'作为评价函数运行网格搜索，并将结果存储在grid_obj中。\n",
    "- 用训练集（X_train, y_train）训练grid search object,并将结果存储在`grid_fit`中。\n",
    "\n",
    "**注意：** 取决于你选择的参数列表，下面实现的代码可能需要花一些时间运行！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO：导入'GridSearchCV', 'make_scorer'和其他一些需要的库\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO：初始化分类器\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# TODO：创建你希望调节的参数列表#题目要求不调整max_depth参数\n",
    "parameters = {'criterion':['gini','entropy'],'min_samples_split':range(2,10),'min_samples_leaf':range(1,10),'max_depth':range(1,11)}\n",
    "\n",
    "# TODO：创建一个fbeta_score打分对象\n",
    "scorer =  make_scorer(fbeta_score, beta=0.5)\n",
    "\n",
    "# TODO：在分类器上使用网格搜索，使用'scorer'作为评价函数\n",
    "grid_obj = GridSearchCV(clf, parameters,scorer)\n",
    "\n",
    "start_train = time() # 获得程序开始时间\n",
    "grid_obj.fit(X_train,y_train)\n",
    "end_train = time() # 获得程序结束时间\n",
    "\n",
    "# TODO：用训练数据拟合网格搜索对象并找到最佳参数\n",
    "\n",
    "# 得到estimator\n",
    "best_clf = grid_obj.best_estimator_\n",
    "print grid_obj.best_params_\n",
    "# 使用调过优的模型做预测\n",
    "start_pred = time() # 获得程序开始时间\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "end_pred = time() # 获得程序结束时间\n",
    "\n",
    "# 使用没有调过优的模型做预测\n",
    "start_train = time() # 获得程序开始时间\n",
    "clf = clf.fit(X_train, y_train)\n",
    "end_train = time() # 获得程序结束时间\n",
    "\n",
    "start_pred = time() # 获得程序开始时间\n",
    "predictions = clf.predict(X_test)\n",
    "end_pred = time() # 获得程序结束时间\n",
    "\n",
    "\n",
    "# 汇报调参前和调参后的分数\n",
    "print \"Unoptimized model\\n------\"\n",
    "print \"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions))\n",
    "print \"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5))\n",
    "print \"Train_time on training data: {:.4f}\".format(end_train-start_train)\n",
    "print \"pred_time on testing data: {:.4f}\".format(end_pred-start_pred)\n",
    "print \"\\nOptimized Model\\n------\"\n",
    "print \"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions))\n",
    "print \"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5))\n",
    "print \"Train_time on training data: {:.4f}\".format(end_train-start_train)\n",
    "print \"pred_time on testing data: {:.4f}\".format(end_pred-start_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 5 - 最终模型评估\n",
    "\n",
    "_你的最优模型在测试数据上的准确率和F-score是多少？这些分数比没有优化的模型好还是差？你优化的结果相比于你在**问题 1**中得到的朴素预测器怎么样？_  \n",
    "**注意：**请在下面的表格中填写你的结果，然后在答案框中提供讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结果:\n",
    "\n",
    "|     评价指标     | 基准预测器 | 未优化的模型 | 优化的模型 |\n",
    "| :------------: | :-----------------: | :---------------: | :-------------: | \n",
    "| 准确率       |      0.2478         |    0.8187          |    0.8554        |\n",
    "| F-score         |       0.2917         |    0.6282         |    0.7236        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：最优模型的分数比未优化模型的分数好；预测的准确率和F-score得以提升.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 特征的重要性\n",
    "\n",
    "在数据上（比如我们这里使用的人口普查的数据）使用监督学习算法的一个重要的任务是决定哪些特征能够提供最强的预测能力。\n",
    "通过专注于一些少量的有效特征和标签之间的关系，我们能够更加简单地理解这些现象，这在很多情况下都是十分有用的。\n",
    "在这个项目的情境下这表示我们希望选择一小部分特征，这些特征能够在预测被调查者是否年收入大于\\$50,000这个问题上有很强的预测能力。\n",
    "\n",
    "选择一个有`feature_importance_`属性（这是一个根据这个选择的分类器来对特征的重要性进行排序的函数）的scikit学习分类器\n",
    "（例如，AdaBoost，随机森林）。在下一个Python代码单元中用这个分类器拟合训练集数据并使用这个属性来决定这个人口普查数据中最重要的5个特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 6 - 观察特征相关性\n",
    "\n",
    "当**探索数据**的时候，它显示在这个人口普查数据集中每一条记录我们有十三个可用的特征。             \n",
    "_在这十三个记录中，你认为哪五个特征对于预测是最重要的，你会怎样对他们排序？理由是什么？_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：十三个特征分别是：age，workclass\teducation_level，education-num，marital-status，occupation，relationship，race，sex，capital-gain，capital-loss，hours-per-week，native-country\n",
    "\n",
    "我认为按重要性从前到后的前五个特征如下：workclass,occupation,education_level,education-num,hours-per-week\n",
    "因为不同的社会阶层很大程度上都有不同的薪酬水平；不同的职业也会有不同的薪酬水平；不同的受教育水平、受教育年长往往也会决定一个人的薪酬水平；最后，多劳多得，平均工时也会联想到不同的薪酬。\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 - 提取特征重要性\n",
    "\n",
    "选择一个`scikit-learn`中有`feature_importance_`属性的监督学习分类器，这个属性是一个在做预测的时候根据所选择的算法来对特征重要性进行排序的功能。\n",
    "\n",
    "在下面的代码单元中，你将要实现以下功能：\n",
    " - 如果这个模型和你前面使用的三个模型不一样的话从sklearn中导入一个监督学习模型。\n",
    " - 在整个训练集上训练一个监督学习模型。\n",
    " - 使用模型中的`'.feature_importances_'`提取特征的重要性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAFgCAYAAAArYcg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XmYFNXZ9/HvLSCIICjiBiiYICqCgAOyKG6JoHHL44Jr\nxKjEGOLyRF/RJEaNJubRRMQluITgTkSiMYpLVBBlQBYFREAFRUFEFoOyy3K/f5zTQ03Ts8FMz0D9\nPtfV10xXnaq6u6q6+q5zTlWZuyMiIiIi6bFDdQcgIiIiIvmlBFBEREQkZZQAioiIiKSMEkARERGR\nlFECKCIiIpIySgBFREREUkYJYA1nZn3NzM1smZntmjWudhx3UzWFt8USn6tlYthcMxtanTHkKPOA\nma02sx2zhveJ0z6dY5rhZrbYzKyC8WzRtjSzo+O0PyijXGMzu8nMOlV0GaXM82Qze9/M1sQYGlfW\nvHMsy0t4PZ4oM9/MHq6k5R1bke0Rl50rvtGJMuPN7OXKiK8CcQ2LccwpYfwf4/j1VbDs2nGf61nO\n8pdlrbvlZvZeHF7lv1dmdruZrUm8rxfjGFDB+VxjZqeUNf98yLFOk68jqmiZZ5jZFVUxb6k8tas7\nACm3RsB1QIUORNuYHwPfVncQWcYA/YAuwNuJ4T2BVcCROaY5EnjLK36TzW7A/C0JspwaA7+Ly3h3\na2dmZrWBJ4BC4BfAd8DyrZ1vGYYCD2QNW5z4/2Tgm0pa1rHAr4GbKjDNSOD3WcOS+/TFwIatC2uL\nrAD2N7Me7j42MzAmVecTtlv9KlhubcI+t57wXSqvUwjbtRFwDvBXYDfgD5UdYBnWEr6Xn1dwumuA\nF4Dns4bfB/yzEuLaEpl1mvRBFS3rDKAAGFRF85dKoARw2/Eq8Eszu8vdv6qKBZhZXXdfWxXzLg93\nf6+6ll2KN+PfnmyeAD4EXGlmB7j7RwBm1gbYk4r92AHg7uO3MtZ8awY0BJ529wp/3mxmVgswdy+t\nJuqL0tZTefahKt7PF5cRX1X94JblK2AqcAEwNjH8WGBv4Eng3GqIqyTvuXvmZOgVMzsAuIoSEsBY\n217H3b+rzCDiSVylfS/dfR4wr7LmV0HJdbrNKefxQSpATcDbjlvj39+UVdDMupjZa2a2wsxWmtnr\nZtYlq8zQ2GTVzcwKzWw18H9x3Fwze9zMLjCzD2MT6Ftm1trMdo7NokvN7Csz+3OsCcrMt56Z3WVm\n0+PyF5rZv83swHLEXdQEbGYtS2m2GJ2YpraZXW9ms8xsrZktiDHVy5r3/mb2opmtstA8ezdQt6yY\n4gHzU0LCl5nXbkBbwo/mZ8lxif/fTAzDzPqZ2dTYVLrEzP4W55Mss1kTsJmdEz/bGgtNraeY2ejk\nOkiob2b3xvkviduwcWZ9xs8B8FBiXfaN43vF/eCbuN0+NLMbS1ovMc658e3fktvFgqvjPL4zsy9j\nXLvk+Ly3mdkAM/uUUIPYrqRllodlNQGb2SVxOT3MbISZfUNMgMysa/yefB33izlmdk8cdyuh9i/Z\n9LzVPzyWaAI2s/3MbKOZ9ctR7sa4zRsnhvUxswkx1v9aaNptVoHFPwqcZcW7M/wEeB1YkCOGuhaa\nLD+L2/FTC825ye97HQtNyJ8k9u23zOzw+B1cHYv+PrEet6QVYxLQNLMPxePKwxaaNz8C1gHHxXEN\n4zEgE/ccM/t/ZsW7ZFg4ThbGuOflistKaAI2s8PM7Pm476w2s5lmdk0mNsJJ4MWJzzw4jstuYp5t\nZk/mWG7PON0JWct8wUJ3oNVmNsbMum3BuszJzPY0s4fi93Wtmc0ws4uyyuwdy8yOMXxuZo+a2V6J\nMsOAPsD3Ep9/VhyXaY7eK2u+JTW932hmvzWzzwjHh9YViLWZmT2RKLMgbrNiXanSTDWA244vgXuB\nq8zsTnf/LFchM2tPSD5mAH0BJzQbv2lmXd19aqJ4I2AYcCdwA5sO1hASme8Rmp13BAYCI4BPgNnA\n2bHMb4A5wP1xurrALsAfgS+AXYHLgXFmdpC7L6zA580+uLUFHgRmJoY9Tmj2+xOhKfIgQhNcS+D0\nuE52BP4D7ERoqlwE/Az4n3LGMgb4HzOr5e4bCE28qwjNqG8R1kMm6ehJaIIsWs9mdjvwK0JzyLWE\nmrNbgUPMrHuc52bM7IeEJtbngf8FmhK2Qz3goxyT3E1odjoXaENI6DcAFxLW5/8Qmp/+yKamqTlm\ntn98/wxwC5sOtPuXsk4eBqYDw+NneZFNTZ23AdcTmrv+DRxM2CaHmtlR7r4xMZ++hH3qGmAlORKR\nLJZMQADKWSPwFCFh/ytQy8waAS8B4whJ0ArCPtM1lh9M2E592bQflqdJf7P4gA25ugO4+2dmNobQ\nBPtg1ujzgX+7+7I406uAvxBqnX9HaM6/BRhlZh3cfVU5YnuasI+cBPzTzHYm7BM/I3fi/RThu/V7\nQi1YT+C3wL7AT2OZGwnfqesJ+0MjQneJ3QjNp0cRjkcPEJrvoeLNqRD2xe8ofow6AegcY1oKzI7f\n9deAVjHumUAPwj7aiE1J/V6x3GeEWtENhGPdPmUFYqHf3Gtx3lcQ9tk28QVwIuF48zbhuwahBjaX\nx4FrzayBu69IDL8AWEho+cHMugKjCNvhYmAN0B94w8y6uPv7ZcVN2O+T++bGzHcxJkXj4vDfELbR\njwgnd7Xd/aE4bnfC9/R6wnG0GeGYNsbM2rr7ujh9E+BA4Mw4XXK7VcTPgA8Jtb9rgEUViHVYjON/\nCb9FewE/JBw/BcDd9arBLzYlcd8nHFSXAUPiuNpx3E2J8s/EMo0Tw3YBvgb+mRg2NE57ao5lzo3l\nGyWGXRHLP5xV9l1gVCnx1yL0LVoOXJ3jc7XMWu7QEubTlJAoFAL14rAj4zx+klX2vDi8Q3x/aXzf\nNVFmB0L/l2IxlLDsi2K5gvj+z8Br8f9+wNxE2c+AFxPvWxJ+XG7MmmePOM/TEsOyt2Uh4UfVEsMO\ni+VGJ4YdHYc9krWMewkHTUvE4sAlWeXOiMN3qeC++f04Xd/EsMwP/9CssufHsqdkfd4FwE7lXJ6X\n8Pp+osz85D4KXBLL3JE1r65x+MGlLO9WYitgOeObX0J8RyfKjAdeTry/GNhI8e9BJrZT4vvGhB/d\n+7OWdwChb91lZcQ1DJgd/38aeC7+/xNC0l4fuB1Yn5imIMYwINc6AdrE968BT5ay7Hqx/G/KuQ4v\ni+X3IxzfmgC/jOtoWKLcQsIxZfes6S+NZQ/PGv57QhLSOPEdXg3slSjTiHDsXJMj/gGJYRMIx6J6\npXyOhWQdK+Pw27Pm/704/wuzlrkM+Eti2FjCSWXtxLA6hJPvYSXFkbVOs1+vJcrcFvexllnTPkb4\nju5QwrxrE04WHTgh1z5XQix7ZQ3PXi+Z9T4X2DGrbJmxAkY4YehX3u9vGl9qAt6GuPvXhAPXTyz0\nNculJ/CCx5qDON23hBqeo7LKriPUGOUyzt2Tnelnxb+vZJWbBbRIDjCzs8zsHTNbRviBWgk0YNMZ\ncoXEs/pn49tT3T3TVNCb8CV/xkJTcO14hvtqHJ9pju0GzPNE3ywPZ76bXcFbgkz/tp6Jv2/F/98G\n9jOzFma2H6F2JNn8+0PCAemJrBjfIfyA5bw60kJ/lwJghMejW4x7MpuacrO9mPX+fUKN7J5lfL4p\nhH1hmIWr9/Yoo3xpuhJqjB/PGj6MsC9k74Mvu3tFageGEGp9kq/y9Kl6Nuv9h4Tk5yEzO8/Mmlcg\nhtK8kCO+yaWUH05ImM9PDLsAWEKooYRwolOfzfehT+KrXFfYRo8CJ5pZE0ICOMJz1x5m5pm9HR/P\nGj8ROM3MbjGz7mZWpwKxlGYuYZ9cAtwF/J2QPCS95e5Lsob1JtSOT85xTKhHqJ2EcEx4yxMtEvF4\n9xKlsNAk3xl4NHEc2mLuPodwondBYvDJhGT0sbjMXWK8/4jvM5/JgTco//b/EcX3y8sT43oTjmXz\ns9bbK4Q+ot+PyzYzu8JCd5QVhG2UaY3YouN7GUb65v06y4w1HjMnAzeYWX8za1sFsW3zlABue+4i\n1M7dUsL43QjNfdkWEppjkxZ7Cc2PwH+z3n9XyvCiKnUzO5lwoJpJaIo8nHCwWcyWV70/DBwC/Mjd\nk1ex7UFINlYSDkSZ16I4vkn8uze5m2DKdTFNPEh/AfQ0swZARzYlgDMJzU9HselAnLwgIpNMzc6K\ncR3hAoom5LY74Qx/UY5xJcX9ddb7zIUOpa53d58N9CIcDx4DFlroq5adrJVHpl9jsX3QQzPt0sR4\ncpUrhy/dfVLWqzwXdGTH81/gGMK6HAzMiz9qp1UwnmxLc8RX4pXR8eTsX8QEMCZQfQi1Outiscw+\n9Dab70OtKXkfyuVlwnf4GsLnf7SEcpntlN1lY2HW+JsINTJnEGqplsS+WVvbzyqTrBwI7OzuFydP\naqNc+84ehEQkez1lvpNbe0zITF+ZF1M8Bhxjm/pzXgBM900XNDUl1Gjdxuaf6xLKv/2nZe2XyW4k\newDH55j/Y3F8ZhnXELqhvEi4a0MXNp3UVUXTaknbuDyx/piwv/8amG6hf/D1ZhW7Pdf2TH0AtzHu\nvsLM/kioCbwjR5GvCX0dsu3F5smb5yi3tc4mVP33zQyIP2rZP/zlYmY3EG4DcYK7z8wavZTQxJnr\nViywqT/Zl4T+g9nKqhlLGkM46BxBaGIaD6F90MzeJiR/RugbmKzxWRr/Hs/m6z85PtsSwkEtV23c\nnmxZP6oSufsoQn+yuoTm6VuAF82sZY5altJkktC9SNxiIp6hN2HzJLUq9sFcNluOu79L6NtZm5Bs\n/JpQm3yIu8/KLl+FHgP6mFlnQmLShE0/ZrBpHzkX+DjH9OW+dZK7rzezp4D/R0hiRpdQNLOd9iSc\n/GTslRwfk+/bgNvMbG/CrUb+TDgxu7C8ceUwzcu+YjXXvrOUULt7fo5xEGpMIRwTcn3/yzomZLZF\nRS6+Kcs/CH0zzzWzIYQaruTFfplt8WdCTXq2yvgOLSWcpF5bwvjM9+FsQq1c0UUxZnZQBZaTqTXd\nMWt4SUlsSdu4zFhj7e5lwGVmdjChK88fCCcxf69AzNstJYDbpvsJHVtvzTHuTUITT8NMzYOZNSQ0\nK4zOQ2z1CU19SRcQ+gJWiJmdTviMl7n7azmKvEzouN3I3V8vZVbjgIviRTDj47x3AM6qQDhjCIno\nz4F3s5rN3iaciRtQmKi5gdAZfCOwr7v/p7wLc/cNZjYJON3Mbso0A5vZYYQO7luSAGZqynYqZblr\nCR3LGxBqploRktHyGk+oFT6bcHVpRh/C8WZ0BeaVF7F2cpyFq55/RLiQaBZxfZnZThVspq6oVwg1\nvRcQEsAP3X1CYvwYQn+1/d39qUpY3sOE/qAvJrsXZMl0YzibkHhknJc1voi7fwk8YGanEmrsIewL\nTin7XCV7mZBA/TfW3JdkHHC5me2VaQaOFwadUMo0uPsyM5tA6IZzeym1z2sp52d29/+a2QuE7b+K\ncKx8Imv8O0B74NpSttnWeJl4QVbsalSS+oQT06SLcpQr6fNnLl48hHgMiyedx1VBrEXcfQbhYpvL\n2bRvpp4SwG2Qu681s1vY/MpBCJ2dTwJeN7M/EQ6+1xG+uCU1G1emlwl9gu4i9IcqIHTizm6+KVW8\nMvVRQt+dafEquIxv3X2Gu4+OtRnPmNlfCJ2zNxJ+3E4ErovNHI8QroT+Z6xRXEQ4Myx2W5IyZJqQ\nTqb4DyKE5uBMbWyxPlPuPiduh3tjv803CWfBLQj9Ax+OtW+5/C5+/mfN7EFCs/BNhDPYjSVMU5qv\nCGfPZ5vZNELT+aeEK/V6Em5iPC8u53pCDer0iizA3b82sz8D15vZyjjPgwiJ/Nts3k+xWsQk5afA\nc4T+Zg0IVxp+S+ifCeFKeoBrzOxVwkUSpfXn2yKJWrnzYxy/zxr/tYXbkPzZzPYhJIzLCbVQxwAv\nufszFVjedKDUpm53n2xmzwJ/sHA7lwmEmvbrgb/7pvtevkRYX+8RvuMFhHsL3hXns9HMPgRONbM3\nCFfIz/fy3w2gov5OqHkcFffD6YR+sN8n1E72it1e7iBcMPKfeCxdTzhGLKfspsz/JZzcjI3HuQVx\n/ge5+//GMjMIzbonEo43i9y9tJO2xwj9VK8H3nD3L7LGX0Xo7zfSwq2yFhKahguAde7+2zJiLsv/\nEZrx3zazgYR+fQ0J393D3f30WO5lwv1o/x/hAsBe5N6XZhCS5IuBacAqD/fAHEs4xtwVE7+NhN+H\ninRHKzNWM9uTcAL7JKFGeEOcZifCSbmArgKu6S8SVwFnDa9N2PGLXTkaxx1OuDpvBeFH/nWgS1aZ\noYQDca5lzgUezxp2dFzWD0qbD+GLfCvhoLiKkPB0JOsKX8q4CjixvFyv0VnLu5JwhdwaNt2C5f8o\nfhXz/oRkZBWhP+LdhFsMFIuhjG2xiKwrWePwOnE9O3BUCdNeQKgdWxm3y0zCVbrNE2VybctzCQew\ntYQm1R8TfmyfLce2ybWOTyMcnNfFcX0JHcz/RTgwryU0jw0nXulZyvrY7CrgONyAq2Pc38X53UfW\nVcZx2lsr8F0oszwlXwXcMqvcQYSLgD6N+80iQnJakPUdGxz3l40krpItZdlDyyhT7CrgxPDM1d0b\ngf1KmPZUwvdpedyPPybU5pW1nXJekZlV5vbsz0dInG4n1NR8F9fVTRS/EvV6QgL4dYxpFqH5slbW\n/jkl7lvFrqjNEUfmKtHmZcSb8yrbOK4+4Rj0UVzm0hjjjRS/or4L4QKMtXHfH0DJV6NmXw3dmXA8\n+SZ+7hnA/ybGtyMkO6vi9IMT63lNjph3JNS0O1l3Ncia5/C4P2ZifhY4vox1Vd512oRwq6rMPfe+\nivvb5YkyDQi3IlpMOFl6jnA1erF1RDi5Hk44KXBgVmLcoYST5hWEY37/UtZ7zqvHy4oV2DnGOSMu\n5xvCd+/M0tZB2l6Z20OIyDYgXq06G7jN3bMfOSYiIlIuSgBFaigz24lw89/XCLUD+xM67+8JtPXQ\n50pERKTC1AdQpObaQLjq8l5Ck8dKQtPJmUr+RERka+SlBjBe2n4SoSPsZlfgxPvy3E3ouL+K0Kfo\n3SoPTERERCSF8nUj6KGES/NLcgLhhqatCY/W+mseYhIRERFJpbw0Abv7GDNrWUqRUwmP1nFgvJk1\nNrO9y2rm2n333b1ly9JmKyIiIpIekydPXuLuTcsqV1P6ADaj+PM858dhpSaALVu2ZNKkSVUZl4iI\niMg2w8w+K7vUNvgsYDPrZ2aTzGzS4sWLy55ARERERIqpKQngF4QnI2Q0p/jzJ4u4+4PuXuDuBU2b\nllnDKSIiIiJZakoC+DzhsTEWH/n1jW5zISIiIlI18tIHMD7n8mhgdzObT3jGaR0Adx9MeKTOiYQn\nHKwi98OlRWQrrVu3jvnz57NmzZrqDkWkTPXq1aN58+bUqVOnukMR2e7k6yrgc8oY78Av8hGLSJrN\nnz+fhg0b0rJlS8LtN0VqJndn6dKlzJ8/n1atWlV3OCLbnZrSBCwiebBmzRqaNGmi5E9qPDOjSZMm\nqq0WqSJKAEVSRsmfbCu0r4pUHSWAIiIiIilTU24ELSLVwB6p3BoWv7DsZ4vXqlWLdu3aFb1/7rnn\nqOgTfZYtW8aTTz7J5ZdfXtEQy+TuNG3alI8//phdd92VL7/8kn322Ye33nqLI444AoCmTZsya9Ys\nmjRpknMezz//PDNmzGDAgAElLmf06NHceeedvPDCC5uNGzhwIP369aN+/fqV86FERLKoBlBE8mqn\nnXZiypQpRa8teZzjsmXLuP/++ys83YYNG8osY2Z07dqVcePGAVBYWEjHjh0pLCwE4MMPP6RJkyYl\nJn8Ap5xySqnJX1kGDhzIqlWrtnh6EZGyKAEUkWq3YcMGrr32Wjp37kz79u154IEHAFixYgXHHXcc\nnTp1ol27dvzrX/8CYMCAAcyZM4cOHTpw7bXXMnr0aE466aSi+fXv35+hQ4cC4ZGR1113HZ06dWL4\n8OHMmTOH3r17c9hhh3HkkUcya9aszeLp3r17UcJXWFjI1VdfXSwh7NGjBwCLFy/m9NNPp3PnznTu\n3JmxY8cCMHToUPr37w/AnDlz6Nq1K+3ateM3v/kNDRo0KFrOihUrOOOMMzjwwAM577zzcHcGDRrE\nggULOOaYYzjmmGMqczWLiBRRE7CI5NXq1avp0KEDAK1ateLZZ5/lb3/7G40aNWLixImsXbuWHj16\ncPzxx9OiRQueffZZdtllF5YsWULXrl055ZRTuP3225k+fTpTpkwBQnNqaZo0acK7774LwHHHHcfg\nwYNp3bo177zzDpdffjlvvPFGsfI9evTg5ptvBmDChAncfPPN3H333UBIALt37w7AlVdeydVXX80R\nRxzB559/Tq9evZg5c2axeV155ZVceeWVnHPOOQwePLjYuPfee48PPviAffbZhx49ejB27FiuuOIK\n/vKXvzBq1Ch23333LVjDIiJlUwIoInmVaQJOevXVV5k2bRrPPPMMAN988w0ff/wxzZs354YbbmDM\nmDHssMMOfPHFF3z11VcVXmafPn2AUONWWFjImWeeWTRu7dq1m5Xv3Lkz7733HitXrmTdunU0aNCA\n/fffn9mzZ1NYWMivfvUrAF577TVmzJhRNN23337LihUris1r3LhxPPfccwCce+65XHPNNUXjunTp\nQvPmzQHo0KEDc+fOLepnKFIT2SOPVNm8/cILq2zesjklgCJS7dyde+65h169ehUbPnToUBYvXszk\nyZOpU6cOLVu2zHlfuNq1a7Nx48ai99lldt55ZwA2btxI48aNN0tAs9WvX5/WrVszZMgQOnXqBEDX\nrl0ZOXIkixYtok2bNkXzGz9+PPXq1av4hwbq1q1b9H+tWrVYv379Fs1HRKSi1AdQRKpdr169+Otf\n/8q6desA+Oijj1i5ciXffPMNe+yxB3Xq1GHUqFF89tlnADRs2JDly5cXTb/ffvsxY8YM1q5dy7Jl\ny3j99ddzLmeXXXahVatWDB8+HAiJ59SpU3OW7d69OwMHDqRbt24AdOvWjbvvvpuuXbsW3Z/u+OOP\n55577imaJldi2bVrV0aMGAHAsGHDyrU+sj+fiEhlUw2gSIqV57Yt+XDJJZcwd+5cOnXqVHQbluee\ne47zzjuPk08+mXbt2lFQUMCBBx4IhD59PXr04JBDDuGEE07gjjvu4KyzzuKQQw6hVatWdOzYscRl\nPfHEE/z85z/n1ltvZd26dZx99tkceuihm5Xr0aMHd999d1EC2KlTJ+bPn88ll1xSVGbQoEH84he/\noH379qxfv56ePXtu1s9v4MCBnH/++dx222307t2bRo0albk++vXrR+/evdlnn30YNWpUudahiEhF\nWHgM77apoKDAJ02aVN1hiGwzZs6cyUEHHVTdYaTKqlWr2GmnnTAzhg0bxlNPPVV0NbOUTftszaI+\ngDWfmU1294KyyqkGUESkCk2ePJn+/fvj7jRu3JghQ4ZUd0giIkoARUSq0pFHHlliP0MRkeqii0BE\nREREUkYJoIiIiEjKKAEUERERSRklgCIiIiIpo4tARFKssm/pUJ7bOCxcuJCrrrqKiRMn0rhxY/bc\nc08GDhzIAQccUKmxJB199NHceeedFBSUfGeEgQMH0q9fP+rXrw/AiSeeyJNPPknjxo23atktW7ak\nYcOG1KpVC4D777+/6FnCFfGHP/yBG264YatiKUnHjh35+9//TocOHVi/fj2NGzdm8ODBnH/++QAc\ndthhPPTQQ0VPRck2adIkHn30UQYNGlTiMubOnctJJ53E9OnTNxs3dOhQjj/+ePbZZ5/K+UAiUiYl\ngCKSN+7Oj3/8Yy688MKip2JMnTqVr776qkoTwPLI3LA5kwCOHDmy0uY9atQodt99962ax5YkgOvX\nr6d27bIP8z169KCwsJAOHTowdepUDjjgAAoLCzn//PNZuXIlc+bMyXmz7IyCgoISk+tJS5YAsODr\nr1m9YUPR+6R7HnqIHZo35+Add9xs3JIVKzi4hBMV3TdOZMupCVhE8mbUqFHUqVOHyy67rGjYoYce\nypFHHsno0aM56aSTiob379+foUOHAqEW7frrr6dDhw4UFBTw7rvv0qtXL773ve8VPXmjtOmTfv7z\nn1NQUEDbtm353e9+B4QneixYsIBjjjmGY445pmiZS5YsYcCAAdx3331F0990003ceeedANxxxx10\n7tyZ9u3bF82rvEqa9rTTTuOwww6jbdu2PPjggwAMGDCA1atX06FDB8477zzmzp3LIYccUjTNnXfe\nyU033QSE2s6rrrqKgoIC7r77bhYvXszpp59O586d6dy5M2PHjt0slu7du1NYWAhAYWEhl112WdFj\n7SZMmMBhhx1GrVq1WLlyJT/96U/p0qULHTt2LLqhdXLdL168mB/+8Ie0bduWSy65hJM7dmTZ0qUA\nbNywgVuvvpqzjjiC/meeyZrVq3n9+eeZOWUKv73sMs49+mjWrF5dofUoIltGCaCI5M306dM57LDD\ntmjafffdlylTpnDkkUfSt29fnnnmGcaPH1/hxOu2225j0qRJTJs2jTfffJNp06ZxxRVXFD12LfvR\na3369OHpp58uev/000/Tp08fXn31VT7++GMmTJjAlClTmDx5MmPGjMm5zGOOOYYOHTpw+OGHA5Q6\n7ZAhQ5g8eTKTJk1i0KBBLF26lNtvv52ddtqJKVOm8MQTT5T5Gb/77jsmTZrEr371K6688kquvvpq\nJk6cyIgRI4o9yi4jUwMIIQHs2bMndevWZfny5RQWFhY1Wd92220ce+yxTJgwgVGjRnHttdeycuXK\nYvO6+eabOfbYY/nggw8444wzWDh/ftG4eZ98wpk//SlPv/02DXfZhTdeeIHjTjmFgzp04PeDB/Pk\n6NHU22mnMj+fiGw9NQGLyDbhlFNOAaBdu3asWLGChg0b0rBhQ+rWrcuyZcvKPZ+nn36aBx98kPXr\n1/Pll18yY8YM2rdvX2L5jh07smjRIhYsWMDixYvZddddadGiBXfffTevvvpq0XOHV6xYwccff0zP\nnj03m0c756glAAAgAElEQVR2E/Crr75a4rSDBg3i2WefBWDevHl8/PHHNGnSpNyfD0LSmvHaa68x\nY8aMovfffvstK1asoEGDBkXD9ttvP7777jsWLlzIrFmzaNOmDZ07d+add96hsLCQX/7yl0VxP//8\n80U1oGvWrOHzzz8vtuy33367KP7evXuzS6IP5T777kubdu0AOPDQQ/kya1oRyR8lgCKSN23btuWZ\nZ57JOa527dps3Lix6P2aNWuKja9bty4AO+ywQ9H/mfeZvm6lTQ/w6aefcueddzJx4kR23XVX+vbt\nm7NctjPPPJNnnnmGhQsXFiVX7s7111/Pz372szKnz1bStKNHj+a1115j3Lhx1K9fn6OPPjpnfGV9\n1p133rno/40bNzJ+/Hjq1atXakzdu3dn+PDh7L333pgZXbt2ZezYsUyYMIFu3boVxT1ixAjatGlT\nbNqvvvqqXJ+7TnK71arFhnKsexGpGmoCFpG8OfbYY1m7dm1R3zaAadOm8dZbb7HffvsxY8YM1q5d\ny7Jly3j99dcrNO/yTP/tt9+y884706hRI7766iteeumlonENGzZk+fLlOefdp08fhg0bxjPPPMOZ\nZ54JQK9evRgyZAgrVqwA4IsvvmDRokXlirWkab/55ht23XVX6tevz6xZsxg/fnzRNHXq1GHdunUA\n7LnnnixatIilS5eydu1aXnjhhRKXdfzxx3PPPfcUvc/07cvWvXt3Bg4cWJTsdevWjUcffZS99tqL\nRo0aFcV9zz334O4AvPfee5vNp0ePHkVN5q+++irflqN2tn6DBqyK60JE8kM1gCIplu+rKM2MZ599\nlquuuoo//elP1KtXj5YtWzJw4EBatGjBWWedxSGHHEKrVq2KmkfLqzzTH3rooXTs2JEDDzyQFi1a\n0KNHj6Jx/fr1o3fv3kV9AZPatm3L8uXLadasGXvvvTcQEquZM2cWJUwNGjTg8ccfZ4899igz1pKm\n7d27N4MHD+aggw6iTZs2dO3atVh87du3p1OnTjzxxBPceOONdOnShWbNmnHggQeWuKxBgwbxi1/8\ngvbt27N+/Xp69uxZdOFMUo8ePbj66quLYtp7773ZsGFDsVvW/Pa3v+Wqq66iffv2bNy4kVatWm2W\nfP7ud7/jnHPO4bHHHqNbt2402WOPkOBl9RVMOvnss/njNddQt149hrz0kvoBiuSBZc7ktkUFBQU+\nadKk6g5DZJsxc+ZMDjrooOoOQ7Zja9eupVatWtSuXZtx48Zx4aWX8uTo0Vs8vyWffsoJiT6MSboN\nTP5V9r1Dk7Q9K4eZTXb3km96GqkGUEREKs3nn3/OWWedxcaNG9lxxx359V13VXdIIpKDEkAREak0\nrVu3LtY3MNeNn0Wk+ukiEJGU2Za7fUjKuLOx7FIisgWUAIqkSL169Vi6dKmSQKn53Fm/fDmz9WQQ\nkSqhJmCRFGnevDnz589n8eLF1R2KpMSSLby9y0Zg9urV3KSbRYtUCSWAIilSp04dWrVqVd1hSIoc\nXIVXjYrIllMTsIiIiEjKKAEUERERSRklgCIiIiIpowRQREREJGWUAIqIiIikjBJAERERkZTRbWBE\npMbQg+ZFRPJDNYAiIiIiKaMEUERERCRllACKiIiIpIwSQBEREZGUUQIoIiIikjJKAEVERERSRgmg\niIiISMooARQRERFJmbwlgGbW28w+NLPZZjYgx/hGZvZvM5tqZh+Y2UX5ik1EREQkTfKSAJpZLeA+\n4ATgYOAcMzs4q9gvgBnufihwNPBnM9sxH/GJiIiIpEm+agC7ALPd/RN3/w4YBpyaVcaBhmZmQAPg\na2B9nuITERERSY18JYDNgHmJ9/PjsKR7gYOABcD7wJXuvjF7RmbWz8wmmdmkxYsXV1W8IiIiItut\nmnQRSC9gCrAP0AG418x2yS7k7g+6e4G7FzRt2jTfMYqIiIhs8/KVAH4BtEi8bx6HJV0E/NOD2cCn\nwIF5ik9EREQkNfKVAE4EWptZq3hhx9nA81llPgeOAzCzPYE2wCd5ik9EREQkNWrnYyHuvt7M+gOv\nALWAIe7+gZldFscPBn4PDDWz9wEDrnP3JfmIT0RERCRN8pIAArj7SGBk1rDBif8XAMfnKx4RERGR\ntKpJF4GIiIiISB4oARQRERFJGSWAIiIiIimjBFBEREQkZZQAioiIiKSMEkARERGRlFECKCIiIpIy\nSgBFREREUkYJoIiIiEjKKAEUERERSRklgCIiIiIpowRQREREJGWUAIqIiIikjBJAERERkZRRAigi\nIiKSMkoARURERFJGCaCIiIhIyigBFBEREUkZJYAiIiIiKaMEUERERCRllACKiIiIpIwSQBEREZGU\nUQIoIiIikjJKAEVERERSRgmgiIiISMooARQRERFJGSWAIiIiIimjBFBEREQkZZQAioiIiKSMEkAR\nERGRlFECKCIiIpIySgBFREREUkYJoIiIiEjKKAEUERERSRklgCIiIiIpowRQREREJGWUAIqIiIik\njBJAERERkZRRAigiIiKSMrWrOwCRLWWPPFKl8/cLL6zS+YuIiFQX1QCKiIiIpIwSQBEREZGUUQIo\nIiIikjJKAEVERERSRgmgiIiISMooARQRERFJGSWAIiIiIimjBFBEREQkZfKWAJpZbzP70Mxmm9mA\nEsocbWZTzOwDM3szX7GJiIiIpElengRiZrWA+4AfAvOBiWb2vLvPSJRpDNwP9Hb3z81sj3zEJiIi\nIpI2+aoB7ALMdvdP3P07YBhwalaZc4F/uvvnAO6+KE+xiYiIiKRKvhLAZsC8xPv5cVjSAcCuZjba\nzCab2U9yzcjM+pnZJDObtHjx4ioKV0RERGT7VZMuAqkNHAb8COgF/NbMDsgu5O4PunuBuxc0bdo0\n3zGKiIiIbPPy0gcQ+AJokXjfPA5Lmg8sdfeVwEozGwMcCnyUnxBFRERE0iFfNYATgdZm1srMdgTO\nBp7PKvMv4Agzq21m9YHDgZl5ik9EREQkNfJSA+ju682sP/AKUAsY4u4fmNllcfxgd59pZi8D04CN\nwMPuPj0f8YmIiIikSb6agHH3kcDIrGGDs97fAdyRr5hERERE0qgmXQQiIiIiInmgBFBEREQkZZQA\nioiIiKRMuRNAMzuzhOFnVF44IiIiIlLVKlID+LcShj9YGYGIiIiISH6UeRWwme0f/93BzFoBlhi9\nP7CmKgITERERkapRntvAzAackPjNyRq3ELipkmMSERERkSpUZgLo7jsAmNmb7n5U1YckIiIiIlWp\n3H0AlfyJiIiIbB/K/SSQ2P/vNqAD0CA5zt33reS4RERERKSKVORRcE8S+gD+ClhVNeGIiIiISFWr\nSALYFujh7hurKhgRERERqXoVuQ/gGKBjVQUiIiIiIvlRag2gmd2SeDsXeNnMniXc/qWIu99Y+aGJ\niIiISFUoqwm4Rdb7F4A6OYaLiIiIyDai1ATQ3S/KVyAiIiIikh8VuQ3M/iWMWgt8qYtDRERERLYN\nFbkKOPNIOAiPhfPEuI1m9jxwubt/VVnBiYiIiEjlq8hVwJcS7gV4AFAPaAM8BlwOtCMkk/dVdoAi\nIiIiUrkqUgN4M/B9d18T3882s8uBj9z9ATPrC3xc2QGKiIiISOWqSA3gDkDLrGH7ArXi/yupWEIp\nIiIiItWgIgnbQOANM/s7MA9oDlwUhwOcCIyr3PBEREREpLKVOwF09/8zs2nAmUAn4EvgYnd/OY5/\nDniuSqIUERERkUpToSbbmOy9XEWxiIiIiEgelPUouF+7+23x/1tKKqdHwYmIiIhsO8qqAWye+F+P\nfxMRERHZDpT1KLifJ/7XY+FEREREtgMV6gNoZgcSLgLZ0937m1kboK67T6uS6ERERESk0pX7PoBm\ndibwFtAM+Ekc3BD4SxXEJSIiIiJVpCI3gr4F+IG7XwZsiMOmAodWelQiIiIiUmUqkgDuAWSaej3x\n13MXFxEREZGaqCIJ4GTggqxhZwMTKi8cEREREalqFbkI5ArgVTO7GNjZzF4BDgCOr5LIRERERKRK\nlJkAmtlZwBh3nxWvAj4JeIHwPOAX3H1FFccoIiIiIpWoPDWAtwLfM7M5wBjgTeBpd/+sSiMTERER\nkSpRZh9Adz+AcOuXXwOrgV8Bc8zsMzN7zMwuqeIYRURERKQSlesiEHdf6O7D3f2X7t4BaArcB/wQ\neKAqAxQRERGRylWui0DMzIAOQM/46g4sAJ4m3BxaRERERLYR5bkI5EWgI/Ah8DbwINDX3ZdXcWwi\nIiIiUgXK0wR8ALAW+BSYA8xW8iciIiKy7SqzBtDdW5vZXsCRhObfq8xsd2Asofn3bXefUrVhioiI\niEhlKVcfQHdfCAyPL8xsV+BS4DeEC0JqVVWAIiIiIlK5tvQikCOAxsAkYEiVRSciIiIila48F4GM\nBLoBOwLvEG4EfS8wzt3XVG14IiIiIlLZylMDOIbwNJCJ7r6uiuMRERERkSpWnotAbs9HICIiIiKS\nH+XqA7i9sEceqcK5963CedccfqFXdwgiIiKylcr1KDgRERER2X7kLQE0s95m9qGZzTazAaWU62xm\n683sjHzFJiIiIpImeUkAzawWcB9wAnAwcI6ZHVxCuT8Br+YjLhEREZE0ylcNYBfCI+Q+cffvgGHA\nqTnK/RIYASzKU1wiIiIiqZOvBLAZMC/xfn4cVsTMmgE/Bv5a2ozMrJ+ZTTKzSYsXL670QEVERES2\ndzXpIpCBwHXuvrG0Qu7+oLsXuHtB06ZN8xSaiIiIyPYjX7eB+QJokXjfPA5LKgCGhafOsTtwopmt\nd/fn8hOiiIiISDrkKwGcCLQ2s1aExO9s4NxkAXdvlfnfzIYCLyj5ExEREal8eUkA3X29mfUHXgFq\nAUPc/QMzuyyOH5yPOEREREQkj08CcfeRwMisYTkTP3fvm4+YRERERNKoJl0EIiIiIiJ5oARQRERE\nJGWUAIqIiIikjBJAERERkZRRAigiIiKSMkoARURERFJGCaCIiIhIyigBFBEREUkZJYAiIiIiKaME\nUERERCRllACKiIiIpIwSQBEREZGUUQIoIiIikjJKAEVERERSRgmgiIiISMooARQRERFJGSWAIiIi\nIimjBFBEREQkZZQAioiIiKSMEkARERGRlFECKCIiIpIySgBFREREUkYJoIiIiEjKKAEUERERSRkl\ngCIiIiIpowRQREREJGWUAIqIiIikjBJAERERkZRRAigiIiKSMkoARURERFJGCaCIiIhIyigBFBER\nEUkZJYAiIiIiKaMEUERERCRllACKiIiIpIwSQBEREZGUUQIoIiIikjJKAEVERERSRgmgiIiISMoo\nARQRERFJGSWAIiIiIimjBFBEREQkZZQAioiIiKSMEkARERGRlKld3QGI1FT2iFV3CHnhF3p1hyAi\nInmmGkARERGRlFECKCIiIpIySgBFREREUiZvCaCZ9TazD81stpkNyDH+PDObZmbvm1mhmR2ar9hE\nRERE0iQvCaCZ1QLuA04ADgbOMbODs4p9Chzl7u2A3wMP5iM2ERERkbTJVw1gF2C2u3/i7t8Bw4BT\nkwXcvdDd/xvfjgea5yk2ERERkVTJ121gmgHzEu/nA4eXUv5i4KVcI8ysH9APYN99962s+ERkO5eW\n2/qAbu0jImWrcReBmNkxhATwulzj3f1Bdy9w94KmTZvmNzgRERGR7UC+agC/AFok3jePw4oxs/bA\nw8AJ7r40T7GJiIiIpEq+agAnAq3NrJWZ7QicDTyfLGBm+wL/BC5w94/yFJeIiIhI6uSlBtDd15tZ\nf+AVoBYwxN0/MLPL4vjBwI1AE+B+MwNY7+4F+YhPREREJE3y9ixgdx8JjMwaNjjx/yXAJfmKR0RE\nRCStatxFICIiIiJStZQAioiIiKSMEkARERGRlFECKCIiIpIySgBFREREUkYJoIiIiEjKKAEUERER\nSRklgCIiIiIpowRQREREJGWUAIqIiIikjBJAERERkZRRAigiIiKSMkoARURERFJGCaCIiIhIyigB\nFBEREUkZJYAiIiIiKaMEUERERCRllACKiIiIpIwSQBEREZGUUQIoIiIikjJKAEVERERSRgmgiIiI\nSMooARQRERFJGSWAIiIiIimjBFBEREQkZZQAioiIiKSMEkARERGRlFECKCIiIpIySgBFREREUkYJ\noIiIiEjKKAEUERERSZna1R2AiIjIlrBHrLpDyAu/0Ks7BNkOqQZQREREJGWUAIqIiIikjBJAERER\nkZRRAigiIiKSMkoARURERFJGCaCIiIhIyigBFBEREUkZ3QdQREREql1a7usINePejqoBFBEREUkZ\nJYAiIiIiKaMEUERERCRllACKiIiIpIwSQBEREZGUUQIoIiIikjJKAEVERERSRgmgiIiISMrkLQE0\ns95m9qGZzTazATnGm5kNiuOnmVmnfMUmIiIikiZ5SQDNrBZwH3ACcDBwjpkdnFXsBKB1fPUD/pqP\n2ERERETSJl81gF2A2e7+ibt/BwwDTs0qcyrwqAfjgcZmtnee4hMRERFJjXw9C7gZMC/xfj5weDnK\nNAO+TBYys36EGkKAFWb2YeWGWiPtDiyp7iAArG96ntVYxbRNtz/aptsXbc/tT1q26X7lKZSvBLDS\nuPuDwIPVHUc+mdkkdy+o7jik8mibbn+0Tbcv2p7bH23T4vLVBPwF0CLxvnkcVtEyIiIiIrKV8pUA\nTgRam1krM9sROBt4PqvM88BP4tXAXYFv3P3L7BmJiIiIyNbJSxOwu683s/7AK0AtYIi7f2Bml8Xx\ng4GRwInAbGAVcFE+YttGpKrJOyW0Tbc/2qbbF23P7Y+2aYK5e3XHICIiIiJ5pCeBiIiIiKSMEkAR\nERGRlFECKAKYWV8zu7eS53la8ok3ZnaLmf2gMpchVcfM9jGzZ+L/HczsxHJMc7SZvVBJyy8ws0GV\nMS8JKnubmtloM9NtRWSbVKMTwHwcgLN/pLe23JYys7lm9lbWsClmNr0S5j3SzBpXoHyFkiEzOyXX\n852F0wiPPgTA3W9099eqMR6pAHdf4O5nxLcdCBep5XP5k9z9inwuc3tX3dtUpCapsQmgmdXO05e1\n2I90JZTbGg3NrAWAmR1U0YnNrHbWezOzHdz9RHdfVllBZnP359399qqaf2Uws/PNbEJMqh8ws1pm\ndpGZfWRmE4AeibJDzeyMxPsVif+vM7P3zWyqmd0eh11qZhPjsBFmVt/MugOnAHfEZX4vOV8zO87M\n3ovzGmJmdePwuWZ2s5m9G8cdWMLnyVnOzG4ys2sS5aabWcv4mhVj+MjMnjCzH5jZWDP72My6VOoK\nrwHM7CdmNi1ul8fM7GQzeyeu99fMbM9Y7qY4flxcF5fG4S3j+tsRuAXoE7dlHzPrEsu/Z2aFZtam\nHPGcGLfBZDMblDlRLWleyZPZGOOQWOP0iZmlMjGsads0K7Zz4ndxupn9KQ6rFb9z0+O4q+PwK8xs\nRvwswyp3LaWXmT0Xv18fWHhqGGZ2cTzmTTCzhyxWbphZ03i8nhhfPUqf+3bI3SvtBbQEZgFDgY+A\nJ4AfAGOBj4EusVwXYBzwHlAItInD+xLuB/gG8Gac33RgR+BzYDEwBehTyjyOBl4oIb7bgRnANOBO\noDvwNfBpnO/3gEsJ9y2cCowA6pdQbjRQEOe7OzA3/t8WmBDLTQNal3PdzQVuAK6J728BrgOmJ9bt\nW8C78dU98Xnfiuvto1juQ+BR4APCI2HmArvH8ucn4nsAqBWHXxSnnwA8BNxbQpy94/KnAq8nttu9\nQCPgM2CHOHxnwuP96mTNY2fgxTiP6UCfxDr4P+D9GMf3E5/9jbg+Xwf2jcOHAmck5rsi/t0bGBM/\n4/T4mf8NnEDYZxbH+c8DmhL2r7GZz1zKfE8g7Gv14/vd4t8mibK3Ar8sYT5DgTOAenHZB8ThjwJX\nJdZBZvrLgYdL2V82KwfcRNyH4vvpcf21BNYD7QgnfpOBIYARnsP9XGUeC6r7RfgefsSm/X43YFc2\n3fngEuDPiXU2FdiJ8F2eB+wT11nm+9eXxHcC2AWoHf//ATCitONPYpu3iu+fypQrz7xijIVA3Rjj\nUrK+V9v7q6Zt0zhuNFAQ5/054XhSm3C8Og04DPhPonzj+HcBUDc5TK9K2Ucyx+SdCMe+ZoRj5W5A\nHcJvZeY4/yRwRPx/X2Bmdcef71dV1AB+H/gzcGB8nQscAVxDSHAgJIlHuntH4EbgD4npOxF+NI/K\nDHD372K5f7h7B3f/Rxnz2IyZNQF+DLR19/bAre5eSEicro3znQP80907u/uhwEzg4hLKleQy4G53\n70A4MMwvdW0VNwL4n/j/yYSkJWMR8EN370RIgJN9gzoBV7r7AfF9a+B+d2/r7p8l1sFBcdoeMb4N\nwHlmtjdwM6EW7AhKqOk0s6aE5PD0uH7OTI53928ISVdm250EvOLu67Jm1RtY4O6HuvshwMuJcd+4\neztCQjkwDrsHeCRutyeyPnsu58bldgAOBfYEOgPDCcnn14QnzSx198Vx//pHGfOE8KPwd3dfFT/v\n13H4IWb2lpm9D5xH+KEqTRvgU3f/KL5/BOiZGP/P+Hcy4QerJOUtl/Gpu7/v7hsJJwevezj6vV/O\n6bclxwLD3X0JFG2r5sArcTtdS/Ht9C93Xx3LjyKcYJamETDcQheNuyh7mx8IfOLun8b3T23BvF50\n97UxxkWE/TpNato2TeoMjI7Hk/WE41RP4BNgfzO7x8x6A9/G8tOAJ8zsfMKJmVSOK8xsKjCe8GSx\nC4A33f3r+Ds0PFH2B8C9ZjaF8Pu+i5k1yHvE1agqEsDy/MiU9kX7T+KHtTQV/bJ+A6wB/mZm/0O4\n2XQuFf0xzzYOuMHMrgP2c/fVFZh2KfBfMzubkHwmY6wDPBTjGk7xJG1C4ocF4DN3H59j/scRzkgn\nxp3+OGB/4HA2HbxKS4a6AmMyyyphO/2DkGRCeOJLrnm9D/zQzP5kZkfGxDHjqcTfbvH/boSzNYDH\nCElqaSYCF5nZTYQar+8INYJrgI3AakLyu3MJ068nfjfMbAdCDWFphgL9Y+J6M6G2Z2usjX83EG/W\nbmavxKaqh0srl4w9qpejPIT1sDbx/zb3XPAtcA/h7L8d8DOKr5vsG6KWdYPU3wOj4gnMyeTY5iVs\nsy2aV5TcfsltnmY1dZuGBbr/l3ASOppQOZCZ7kfAfYST94mW1X1HKs7MjiYkdd1iBcV7hIqikuwA\ndI2VOh3cvZm7ryil/HanKhLA8vzIlPZFW1nO5VToyxrPyroAzxBqpl7OLh8NpXw/5skf2qIy7v4k\noe/XamCkmR1bzs+T8Q/CgeGprOFXA18RDiYFFE9KstdZSevQCDVpmR2+jbvfVFIgsf/KlPi6pZzx\nPw/0NrPdCMnmG2bWIjGfy2LNVydCInirmd2YmN5L+D+XnImau48hnH1/QdieOxOSxjGxVvDY+H5H\nM2tiZnUoXps5N8YOYVvWif//h5BY1o/L3C0Obwh8GedzXmI+y+O4bB8CLc3s+/H9BYQuDyVy915x\nm11SWrkYe6cYXyegVRnlt1dvAGfGmv/MtmrEpueLX5hV/lQzqxfLH004iUjK3pbJefXNFUDWNvuQ\nUBPUMo7ukyha5rwEqHnbNGkCcJSZ7W5mtYBzgDfNbHdCl5gRwG+ATvFY1cLdRxG6+TQCUlXzVEUa\nAf9191UW+kR3JRz7jzKzXWOSfXqi/KvALzNvzKxDXqOtAarrIpAtOeBt1Zc1Vu02cveRhGTq0BLm\nW94f87lsShKSFwzsT2jqGQT8C2hfzs+X8SyhH9wrWcMbAV/GmtULCI/Uq6jXgTPMbI8Y625mth/w\nDuFLUiwZcvcNiWTxRkK1ek8za5WZPnsB8QxqInA3od/MBnefl5jPYDPbB1jl7o8DdxATlqhP4u+4\n+H8hoTYRwjbJXC09lxyJWvxMX7n7Q4Qz7r0I3QROMrNZhESuFfDXuIyxhBrXjIfi+phKqH1cGT/b\ny4QEd1KsQc1cbPHbuA7HUvyMcxhwbexU/r3EOlpD6HM5PNbobgQGZ6/LLTQC2M3MPgD6E/pMpY67\nfwDcRvgRngr8hdAvbLiZTQaWZE0yjdBMOB74vbsvyBo/Cjg4nsT0IXxH/2hm71GOmrjYEnA58HJc\n/nJCqwQVnVda1bRtmhXbl8CAOM+pwGR3/xehD9roeLx4HLiecOx+PH733wMGeRVepJciLwO1zWwm\nob//eEKO8AdCgj6W8JuR+d5dARRYuBBnBqGGNl28cjtgtiR2sI3vhxI7wVO88203wg/Te4RO83Pj\n8L4U75SbnGY3QmKRuQikpHkcTe5O2HsTdoJphJqnC+PwHoQLQ94jXNzxc8LFHhMIzQtDSyh3YJxX\n9vIHEJq+pxB2yN3Kue7mEjs3l/D5W8flTQX+xKYLE4p93uxtkD3vuO4yF6hMJlSBQ/GLQB6k5ItA\nToifeSqxc3OO7XYGofbuqBLm0Ssuf0rcpgWJOP8Ux01k00Ug+5H7IpA9CV/y7HVyIaED8HuEZDHT\n8f7YON9p8XVKZe7/em2bL7IunKnC5TSIfw24H7i6uj/79vrK1zbVq+a/Et+72oR+9T+u7phqykvP\nApYaw8zmEpLB7DN5kSoT+4qucPc7q3g5VxNOTnYknJxc6vGCIqlc+dqmUvOZ2Z2EvoH1CM2+V7oS\nHwAlgFJzKAEUERHJDyWAVSx2QH49x6jj3H1pvuMRERERUQIoIiIikjI19lFwIiIiIlI1lACKiIiI\npIwSQBEREZGUUQIoIqliZnPNbLWZrUi89tmK+R1tZhV55reISLVTAigiaXSyuzdIvLKfEpE3eg6s\niFQHJYAiIoCZdTWzQjNbZmZT48PlM+MuMrOZZrbczD4xs5/F4TsDLwH7JGsTzWyomd2amL5YLWGs\nhbzOzKYBK82sdpxuhJktNrNPzeyK/H16EUkbJYAiknpm1gx4kfBYx90Iz3keYWZNY5FFwEnALoTH\nJv0UzrwAAAHsSURBVN5lZp3cfSXh8YgLtqA28RzgR0BjwvOg/014rGEz4DjgKjPrVSkfUEQkixJA\nEUmj52JN3zIzew44Hxjp7iPdfaO7/weYBJwI4O4vuvscD94kPFLqyK2MYZC7z3P31UBnoKm73+Lu\n37n7J8BDwNlbuQwRkZzU90RE0ug0d38t88bM7gfONLOTE2XqAKPi+BOA3wEHEE6c6wPvb2UM8xL/\n70doRl6WGFYLeGsrlyEikpMSQBGRkIw95u6XZo8ws7rACOAnwL/cfV2sNbRYJNfjlFYSksSMvXKU\nSU43D/jU3VtvSfAiIhWlJmAREXgc+P/t2y9KhUEUhvHnVfBPuqBFxCUYLG5BQRC7ixCLxbXYDKLR\nZZiMJhVuEgXDrcoYvglfEC+IyfP84sBhZtrLmTmHSfaTLCZZ6YMbW8ASsAy8Ah+9G7g3qn0B1pNM\nRmv3wEGStSQbwOmc/e+AWR8MWe1n2E6y+2c3lKQRA6Ck8lprU+AIOGcIelPgDFhorc2AE+AGeAeO\ngdtR7QNwBTz2P4WbwCXDQMczw3/B6zn7fzIMmewAT8AbcAFMfqqTpN9Ka9+9XkiSJOm/sgMoSZJU\njAFQkiSpGAOgJElSMQZASZKkYgyAkiRJxRgAJUmSijEASpIkFWMAlCRJKuYLUJt1frOJELAAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15abc9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO：导入一个有'feature_importances_'的监督学习模型\n",
    "\n",
    "# TODO：在训练集上训练一个监督学习模型\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini',min_samples_split=7,max_depth=8,min_samples_leaf=1,random_state=0)\n",
    "model = model.fit(X_train,y_train)\n",
    "\n",
    "# TODO： 提取特征重要性\n",
    "importances = model.feature_importances_ \n",
    "\n",
    "# 绘图\n",
    "vs.feature_plot(importances, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 7 - 提取特征重要性\n",
    "观察上面创建的展示五个用于预测被调查者年收入是否大于\\$50,000最相关的特征的可视化图像。\n",
    "_这五个特征和你在**问题 6**中讨论的特征比较怎么样？如果说你的答案和这里的相近，那么这个可视化怎样佐证了你的想法？如果你的选择不相近，那么为什么你觉得这些特征更加相关？_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：这五个特征比起我在问题6中讨论的特征相近，但又有少许出入。由图中特征可知，一个人如果是已婚的话，很大可能收入水平会更高，我认为这是因为大多数人结婚都是建立在一定的物质基础之上的，已婚和收入水平会有很大的相关性；再就是受教育年长，长期锻炼的思维能力确实和个人收入有很大相关性；资本运作能力（capital-gain&capital-loss）也意味着人不只是靠工资收入，更多的也会从资产价格升值和经营现金流增值得到收入，和个人收入水平确实也有很大相关性（而靠贷款投入生产所得的经营利润确实比投资的回报要低）；最后，不同年龄也很可能意味着个人资源的累积程度，也确实可能和个人收入水平有较大相关性。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择\n",
    "\n",
    "如果我们只是用可用特征的一个子集的话模型表现会怎么样？通过使用更少的特征来训练，在评价指标的角度来看我们的期望是训练和预测的时间会更少。从上面的可视化来看，我们可以看到前五个最重要的特征贡献了数据中**所有**特征中超过一半的重要性。这提示我们可以尝试去*减小特征空间*，并简化模型需要学习的信息。下面代码单元将使用你前面发现的优化模型，并*只使用五个最重要的特征*在相同的训练集上训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model trained on full data\n",
      "------\n",
      "Accuracy on testing data: 0.8554\n",
      "F-score on testing data: 0.7236\n",
      "Train_time on testing data: 0.0280\n",
      "\n",
      "Final Model trained on reduced data\n",
      "------\n",
      "Accuracy on testing data: 0.8517\n",
      "F-score on testing data: 0.7182\n",
      "pred_time on testing data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 导入克隆模型的功能\n",
    "from sklearn.base import clone\n",
    "\n",
    "# 减小特征空间\n",
    "X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]\n",
    "X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]]\n",
    "\n",
    "# 在前面的网格搜索的基础上训练一个“最好的”模型\n",
    "start_train = time() # 获得程序开始时间\n",
    "clf = (clone(best_clf)).fit(X_train_reduced, y_train)\n",
    "end_train = time() # 获得程序结束时间\n",
    "\n",
    "# 做一个新的预测\n",
    "start_pred = time() # 获得程序开始时间\n",
    "reduced_predictions = clf.predict(X_test_reduced)\n",
    "end_pred = time() # 获得程序结束时间\n",
    "\n",
    "# 对于每一个版本的数据汇报最终模型的分数\n",
    "print \"Final Model trained on full data\\n------\"\n",
    "print \"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions))\n",
    "print \"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5))\n",
    "print \"Train_time on testing data: {:.4f}\".format(end_train-start_train)\n",
    "\n",
    "\n",
    "print \"\\nFinal Model trained on reduced data\\n------\"\n",
    "print \"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, reduced_predictions))\n",
    "print \"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, reduced_predictions, beta = 0.5))\n",
    "print \"pred_time on testing data: {:.4f}\".format(end_pred-start_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 8 - 特征选择的影响\n",
    "\n",
    "*最终模型在只是用五个特征的数据上和使用所有的特征数据上的F-score和准确率相比怎么样？*  \n",
    "*如果训练时间是一个要考虑的因素，你会考虑使用部分特征的数据作为你的训练集吗？*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：只在五个特征上的F-score和准确率比起使用全部特征数据而言，有所下降。如果考虑训练的时间成本因素，我会考虑使用部分特征的数据作为我的训练集。但我会优先考虑剪枝。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **注意：** 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出**File -> Download as -> HTML (.html)**把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
